{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral Crop Classification: XGBoost and Non-Negative Matrix Factorization\n",
    "\n",
    "In this notebook we'll look to classify spectra of a variety of crops at various stages of development. The dataset contains spectral observations of five different crops across six stages of development taken by the Earth Observing 1 satellite from about 500 - 2300 nm at a spatial resolution of 30 meters. The crops consists of corn, cotton, rice, soybeans, and winter wheat. The six stages are (in sequential order):\n",
    "- emergence / very early vegetative (`Emerge_VEarly`)\n",
    "- early / mid vegetative (`Early_Mid`)\n",
    "- late vegetative (`Late`)\n",
    "- critical (`Critical`)\n",
    "- maturing / senescence (`Mature_Senesc`)\n",
    "- harvest (`Harvest`)\n",
    "\n",
    "We'll begin by performing some exploratory analysis of our dataset, utilizing multiplicative scatter correction to look for outliers and test feature importance using MANOVA, ANOVA, and random forest. We'll then look into the use of Non-Negative Matrix Factorization (NMF) to perform dimensionality reduction. Lastly, we'll utilize XGBoost to create a machine learning classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:47.69521Z",
     "iopub.status.busy": "2022-09-24T22:11:47.693067Z",
     "iopub.status.idle": "2022-09-24T22:11:48.601242Z",
     "shell.execute_reply": "2022-09-24T22:11:48.600184Z",
     "shell.execute_reply.started": "2022-09-24T22:11:47.695142Z"
    }
   },
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import numpy\n",
    "import pandas\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "stage_sequence = CategoricalDtype(['Emerge_VEarly', 'Early_Mid', 'Late', 'Critical', 'Mature_Senesc', 'Harvest'], ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "\n",
    "Let's open our dataset and look at the first few entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:48.606933Z",
     "iopub.status.busy": "2022-09-24T22:11:48.606594Z",
     "iopub.status.idle": "2022-09-24T22:11:48.812218Z",
     "shell.execute_reply": "2022-09-24T22:11:48.811091Z",
     "shell.execute_reply.started": "2022-09-24T22:11:48.606902Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"/kaggle/input/hyperspectral-library-of-agricultural-crops-usgs/GHISACONUS_2008_001_speclib.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectra of each sample is contained in the columns that begin with an `X` followed by a set of digits which represent the wavelength in nanometers (nm). Let's create a variable to hold our wavelength columns and their associated values for easier reference. Let's also cast our `Stage` features to the categorical datatype we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:48.814024Z",
     "iopub.status.busy": "2022-09-24T22:11:48.813669Z",
     "iopub.status.idle": "2022-09-24T22:11:48.830105Z",
     "shell.execute_reply": "2022-09-24T22:11:48.828814Z",
     "shell.execute_reply.started": "2022-09-24T22:11:48.813992Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Stage\"] = df[\"Stage\"].astype(stage_sequence)\n",
    "\n",
    "wave_cols = [c for c in df.columns if re.search(\"X\\d+\", c)]\n",
    "wave = numpy.array([float(x[1:]) for x in wave_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather some metadata for our non-spectral columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:48.835245Z",
     "iopub.status.busy": "2022-09-24T22:11:48.834879Z",
     "iopub.status.idle": "2022-09-24T22:11:48.859591Z",
     "shell.execute_reply": "2022-09-24T22:11:48.858453Z",
     "shell.execute_reply.started": "2022-09-24T22:11:48.83521Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=wave_cols).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AEZ` refers to the agricultural zone which, along with the longitude and latitude, could be useful geospatial features. The `Month` attribute could be useful information as well for distinguishing between stages and crop types.\n",
    "\n",
    "Let's go ahead and sort our data by the crop type and its stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:48.861449Z",
     "iopub.status.busy": "2022-09-24T22:11:48.861114Z",
     "iopub.status.idle": "2022-09-24T22:11:48.889805Z",
     "shell.execute_reply": "2022-09-24T22:11:48.88875Z",
     "shell.execute_reply.started": "2022-09-24T22:11:48.861418Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by=[\"Crop\", \"Stage\"], ignore_index=True)\n",
    "df.drop(columns=wave_cols).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data?\n",
    "\n",
    "There is no missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:48.891402Z",
     "iopub.status.busy": "2022-09-24T22:11:48.89108Z",
     "iopub.status.idle": "2022-09-24T22:11:48.902469Z",
     "shell.execute_reply": "2022-09-24T22:11:48.901257Z",
     "shell.execute_reply.started": "2022-09-24T22:11:48.891373Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class and Stage Distribution\n",
    "\n",
    "We have samples for five types of crop and six stages with some significant class imbalances. Corn and Soybeans are the most populous classes and cover six different stages while the remaining crops are missing some of the stages. There are significantly fewer rice samples that only cover two stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:48.904178Z",
     "iopub.status.busy": "2022-09-24T22:11:48.903865Z",
     "iopub.status.idle": "2022-09-24T22:11:50.075945Z",
     "shell.execute_reply": "2022-09-24T22:11:50.074818Z",
     "shell.execute_reply.started": "2022-09-24T22:11:48.904149Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x=\"Crop\").update_layout(yaxis_title=\"Count\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(df, x=\"Stage\").update_layout(yaxis_title=\"Count\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(df, x=\"Crop\", color=\"Stage\", barmode=\"group\").update_layout(yaxis_title=\"Count\")\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace('_', ' ').capitalize()))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of AEZ there are generally only one or two crops present in our dataset, with zone 9 being the exception with 4 crop types. Corn spans 5 of 7 zones followed by cotton and soybeans at 3/7. The stages are better distributed within each of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:50.07737Z",
     "iopub.status.busy": "2022-09-24T22:11:50.07707Z",
     "iopub.status.idle": "2022-09-24T22:11:50.237302Z",
     "shell.execute_reply": "2022-09-24T22:11:50.236209Z",
     "shell.execute_reply.started": "2022-09-24T22:11:50.077342Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x=\"AEZ\", color=\"Crop\").update_layout(yaxis_title=\"Count\")\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace('_', ' ').capitalize()))\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(df, x=\"AEZ\", color=\"Stage\").update_layout(yaxis_title=\"Count\")\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace('_', ' ').capitalize()))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop types are fairly well spread out across the range of months, but cotton lacks samples for August while the rice samples are only found in August. The month appears to be useful in separating stages with a general progression towards later stages with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:50.239847Z",
     "iopub.status.busy": "2022-09-24T22:11:50.238895Z",
     "iopub.status.idle": "2022-09-24T22:11:50.517259Z",
     "shell.execute_reply": "2022-09-24T22:11:50.516358Z",
     "shell.execute_reply.started": "2022-09-24T22:11:50.23981Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x=\"Month\", color=\"Crop\").update_layout(yaxis_title=\"Count\")\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace('_', ' ').capitalize()))\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(df, x=\"Month\", color=\"Stage\").update_layout(yaxis_title=\"Count\")\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace('_', ' ').capitalize()))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and create a geo-scatterplot to get an idea of where our samples were taken from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:50.519681Z",
     "iopub.status.busy": "2022-09-24T22:11:50.518764Z",
     "iopub.status.idle": "2022-09-24T22:11:50.644338Z",
     "shell.execute_reply": "2022-09-24T22:11:50.64252Z",
     "shell.execute_reply.started": "2022-09-24T22:11:50.519646Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_geo(\n",
    "    data_frame=df,\n",
    "    lat=\"lat\",\n",
    "    lon=\"long\",\n",
    "    color=\"Crop\",\n",
    "    locationmode=\"USA-states\",\n",
    "    category_orders={\"Crop\": list(sorted(set(df[\"Crop\"])))}\n",
    ")\n",
    "fig.update_geos(scope='usa')\n",
    "fig.for_each_trace(lambda t: t.update(name=t.name.replace('_', ' ').capitalize()))\n",
    "fig.update_layout(margin=dict(r=0, t=0, l=0, b=0))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our samples are highly clustered, with the clusters themselves being fairly dispersed. One potential problem with using the latitude and longitude is that models could simply learn to classify based on location. Soybeans and corn are fairly well dispersed in Illinois and South Dakota while Cotton and Winter Wheat are more clustered in Texas and the same for Corn and Rice in California. In Wisconsin and Arizona we only have one crop present in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a choropleth map of the counts in each state based on the crop and stage. To do this, we'll first need to count up the number of samples in each state, which can be done by\n",
    "- Loading a geodataframe containing a polygon representation of the states\n",
    "- Converting our dataframe to a geodataframe using the latitude and longitude for the point positions\n",
    "- Perform a spatial join to link the two geodataframes\n",
    "- Perform a pivot table operation to aggregrate the observations by class type\n",
    "\n",
    "Let's go ahead and download a spatial file representing the US states from the US Census Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:50.646558Z",
     "iopub.status.busy": "2022-09-24T22:11:50.646094Z",
     "iopub.status.idle": "2022-09-24T22:11:54.42Z",
     "shell.execute_reply": "2022-09-24T22:11:54.419155Z",
     "shell.execute_reply.started": "2022-09-24T22:11:50.646499Z"
    }
   },
   "outputs": [],
   "source": [
    "gdf_states = geopandas.read_file(\"https://www2.census.gov/geo/tiger/TIGER2021/STATE/tl_2021_us_state.zip\")\n",
    "#gdf_states.to_file(\"us_states.geojson\", driver='GeoJSON')\n",
    "#gdf_states = geopandas.read_file(\"us_states.geojson\")\n",
    "gdf_states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll convert our crop data frame into a geodataframe in the same coordinate system as our US states shapefile and aggregate the counts across states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:54.422142Z",
     "iopub.status.busy": "2022-09-24T22:11:54.421201Z",
     "iopub.status.idle": "2022-09-24T22:11:54.601559Z",
     "shell.execute_reply": "2022-09-24T22:11:54.600755Z",
     "shell.execute_reply.started": "2022-09-24T22:11:54.422104Z"
    }
   },
   "outputs": [],
   "source": [
    "gdf_crops = geopandas.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=geopandas.points_from_xy(\n",
    "        x=df.long,\n",
    "        y=df.lat,\n",
    "        crs=gdf_states.crs\n",
    "    )\n",
    ")\n",
    "\n",
    "# Spatial Join\n",
    "gdf_sjoin = geopandas.sjoin(gdf_states, gdf_crops)\n",
    "\n",
    "# Pivot Table\n",
    "for feature in [\"Crop\", \"Stage\"]:\n",
    "    df_pivot = pandas.pivot_table(gdf_sjoin, index=\"GEOID\", columns=[feature], aggfunc={feature: len})\n",
    "    df_pivot.columns = df_pivot.columns.droplevel()\n",
    "    gdf_states = gdf_states.merge(df_pivot, how='left', on='GEOID')\n",
    "\n",
    "gdf_states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and visualize our crop distributions with a choropleth. Feel free to use the dropdown menu on the left to change the crop type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:54.608011Z",
     "iopub.status.busy": "2022-09-24T22:11:54.607202Z",
     "iopub.status.idle": "2022-09-24T22:11:54.692953Z",
     "shell.execute_reply": "2022-09-24T22:11:54.692016Z",
     "shell.execute_reply.started": "2022-09-24T22:11:54.607972Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_traces_buttons(feature):\n",
    "    traces = []\n",
    "\n",
    "    categories = df[feature].unique()\n",
    "    for i,c in enumerate(categories):\n",
    "        trace = go.Choropleth(\n",
    "            z=gdf_states[c],\n",
    "            name=\"\",\n",
    "            locations=gdf_states[\"STUSPS\"],\n",
    "            locationmode=\"USA-states\",\n",
    "            colorscale=\"Emrld\",\n",
    "            colorbar_title=\"Count\",\n",
    "            zmin=0,\n",
    "            zmax=(gdf_states[c].max() + 1)//1,\n",
    "            visible = True if i == 0 else False)\n",
    "        traces.append(trace)\n",
    "\n",
    "    buttons = []\n",
    "    for i,c in enumerate(categories):\n",
    "        visible = [True if i==j else False for j,_ in enumerate(categories)]\n",
    "        button = dict(\n",
    "            label=c.capitalize().replace(\"_\", \" \"),\n",
    "            method=\"update\",\n",
    "            args=[dict(visible=visible)]\n",
    "        )\n",
    "        buttons.append(button)\n",
    "        \n",
    "    return traces, buttons\n",
    "\n",
    "def make_choropleth(feature):\n",
    "    traces, buttons = get_traces_buttons(feature)\n",
    "    fig = go.Figure()\n",
    "    fig.add_traces(traces)\n",
    "    fig.update_layout(\n",
    "        geo_scope='usa', \n",
    "        margin=dict(r=0, t=0, l=0, b=0),\n",
    "        updatemenus=[dict(\n",
    "            type=\"dropdown\",\n",
    "            direction=\"down\",\n",
    "            buttons=buttons,\n",
    "            x=0., xanchor=\"left\",\n",
    "            y=1.1, yanchor=\"top\")]\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "make_choropleth(\"Crop\").show()\n",
    "make_choropleth(\"Stage\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crop types tend to be more geographically clustered across a small number of states while the stages tend to be spread out over a larger number of states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectra\n",
    "\n",
    "Let's plot one of our spectra to get an idea of what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:54.694928Z",
     "iopub.status.busy": "2022-09-24T22:11:54.694587Z",
     "iopub.status.idle": "2022-09-24T22:11:54.766406Z",
     "shell.execute_reply": "2022-09-24T22:11:54.76535Z",
     "shell.execute_reply.started": "2022-09-24T22:11:54.694896Z"
    }
   },
   "outputs": [],
   "source": [
    "px.scatter(x=wave, y=df.loc[0,wave_cols]).update_layout(xaxis_title=\"Wavelength (nm)\", yaxis_title=\"Surface Reflectance (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several gaps in the spectrum which are associated with [water vapor](https://www.e-education.psu.edu/meteo300/node/683) absorption in the atmosphere. Let's go ahead and look at how the spectra vary by their stage for each crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:54.768137Z",
     "iopub.status.busy": "2022-09-24T22:11:54.767797Z",
     "iopub.status.idle": "2022-09-24T22:11:59.699243Z",
     "shell.execute_reply": "2022-09-24T22:11:59.698138Z",
     "shell.execute_reply.started": "2022-09-24T22:11:54.768106Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"Crop\": list(sorted(df[\"Crop\"].unique())),\n",
    "    \"Stage\": stage_sequence.categories\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    \"Crop\": {k:px.colors.qualitative.Plotly[i] for i,k in enumerate(labels[\"Crop\"])},\n",
    "    \"Stage\": {k:px.colors.sample_colorscale(\"viridis\", i/(len(labels[\"Stage\"])+1))[0] for i,k in enumerate(labels[\"Stage\"])}\n",
    "}\n",
    "\n",
    "def make_line_plot(feature, n:int=50):\n",
    "    feature2 = [l for l in labels.keys() if l != feature][0]\n",
    "    buttons = {k:[] for k in labels[feature]}\n",
    "    traces = []\n",
    "\n",
    "    for i1, c1 in enumerate(labels[feature]):\n",
    "        sub = df.query(f\"{feature} == '{c1}'\")\n",
    "        \n",
    "        for i2, c2 in enumerate(sub[feature2].unique()):\n",
    "            sub2 = sub.query(f\"{feature2} == '{c2}'\")\n",
    "            if n != None:\n",
    "                sub2 = sub2.sample(frac=1, random_state=11 + 17*i1).reset_index(drop=True).iloc[:n]\n",
    "            \n",
    "            for ir, row in sub2.iterrows():\n",
    "                trace = go.Scatter(\n",
    "                    name=c2,\n",
    "                    legendgroup=c2,\n",
    "                    x=wave,\n",
    "                    y=row[wave_cols],\n",
    "                    line=dict(color=colors[feature2][c2], width=0.5),\n",
    "                    showlegend=True if ir == 0 else False,\n",
    "                    visible=True if i1==0 else False\n",
    "                )\n",
    "                traces.append(trace)\n",
    "                \n",
    "            # Add visibility to button\n",
    "            vtrue = [True for _ in range(len(sub2))]\n",
    "            vfalse = [False for _ in range(len(sub2))]\n",
    "            for k,v in buttons.items():\n",
    "                v.extend(vtrue if k==c1 else vfalse)\n",
    "\n",
    "    buttons = [\n",
    "        dict(\n",
    "            label=k.capitalize().replace(\"_\", \" \"),\n",
    "            method=\"update\",\n",
    "            args=[dict(visible=v)]\n",
    "        )\n",
    "        for k,v in buttons.items()\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_traces(traces)\n",
    "    fig.update_layout(\n",
    "        updatemenus=[dict(\n",
    "            type=\"dropdown\",\n",
    "            direction=\"right\",\n",
    "            buttons=buttons,\n",
    "            x=0., xanchor=\"left\",\n",
    "            y=1, yanchor=\"bottom\")\n",
    "        ],\n",
    "        yaxis_range=[0,100],\n",
    "        xaxis_title=\"Wavelength (nm)\",\n",
    "        yaxis_title=\"Surface Reflectance (%)\",\n",
    "        legend_title=feature2\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "make_line_plot(\"Crop\").show()\n",
    "make_line_plot(\"Stage\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different stages often have distinct patterns that could be used to separate them, such as the large amount of reflection about the 800 - 1200 nm region associated with Early mid - Critical compared to the relative absence in the earliest and latest stages.\n",
    "\n",
    "There tends to be a lot of similarities between the different crop spectral signatures for a given stage which could hamper efforts to distinguish between them. If the stage differences aren't accounted for then one could very well train a model to classify crop types that ends up focusing on the the stage resulting in a poor performance in the real world. Luckily, the crop distribution across the different stages is fairly well balanced in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection\n",
    "\n",
    "Let's go ahead and group our spectra by the crop type and stage and seem how similar the spectra are to one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:11:59.701663Z",
     "iopub.status.busy": "2022-09-24T22:11:59.701053Z",
     "iopub.status.idle": "2022-09-24T22:12:27.552154Z",
     "shell.execute_reply": "2022-09-24T22:12:27.5503Z",
     "shell.execute_reply.started": "2022-09-24T22:11:59.701625Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "crops = labels[\"Crop\"]\n",
    "stages = labels[\"Stage\"]\n",
    "\n",
    "def get_label(crop, stage):\n",
    "    return f\"{crop.capitalize().replace('_', ' ')} ({stage})\"\n",
    "\n",
    "def get_traces_buttons():\n",
    "    traces = []\n",
    "    \n",
    "    sizes = []\n",
    "    categories = []\n",
    "    for crop in crops:\n",
    "        for stage in stages:\n",
    "            # Create traces\n",
    "            subset = df.query(f\"Crop=='{crop}' and Stage == '{stage}'\")\n",
    "\n",
    "            if len(subset) == 0:\n",
    "                continue\n",
    "    \n",
    "            for i, (ir, row) in enumerate(subset.iterrows()):\n",
    "                trace = go.Scatter(\n",
    "                    name=f\"ID={ir}\",\n",
    "                    legendgroup=get_label(crop,stage),\n",
    "                    x=wave,\n",
    "                    y=row[wave_cols],\n",
    "                    mode=\"lines\",\n",
    "                    marker_color=px.colors.sample_colorscale(\"viridis\", i/len(subset))[0],\n",
    "                    marker_opacity=0.5,\n",
    "                    showlegend=False,\n",
    "                )\n",
    "                traces.append(trace)\n",
    "            \n",
    "            sizes.append(len(subset))\n",
    "            categories.append(get_label(crop,stage))\n",
    "        \n",
    "    buttons = []\n",
    "    for i,cat in enumerate(categories):\n",
    "        vis = []\n",
    "        for j,_ in enumerate(categories):\n",
    "            vbool = i == j\n",
    "            vtrue = [vbool for _ in range(sizes[j])]\n",
    "            vis.extend(vtrue)\n",
    "        buttons.append(dict(\n",
    "            label=cat,\n",
    "            method=\"update\",\n",
    "            args=[dict(visible=vis)]\n",
    "        ))\n",
    "\n",
    "    return traces, buttons\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "traces, buttons = get_traces_buttons()\n",
    "fig.add_traces(traces)\n",
    "fig.update_layout(\n",
    "    yaxis_range=[0,100],\n",
    "    xaxis_title=\"Wavelength (nm)\",\n",
    "    yaxis_title=\"Surface Reflectance (%)\",\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type=\"dropdown\",\n",
    "            direction=\"down\",\n",
    "            buttons=buttons,\n",
    "            x=0.,\n",
    "            xanchor=\"left\",\n",
    "            y=1.0,\n",
    "            yanchor=\"bottom\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.for_each_trace(lambda trace: trace.update(visible=True if trace.legendgroup == get_label(crops[0], stages[0]) else False))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soybean (Mature Senesc), in particular, appears to have some misclassifications with some of the spectra looking like they better aline with Soybean (Critical). There does appear to be a rather wide spread in terms of the surface reflectance while the underlying pattern looks to be about the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplicative Scatter Correction (MSC)\n",
    "\n",
    "One means of comparing spectra when there are considerable offsets in the illumance is to apply multiplicative scatter correction (MSC), which is a normalization technique that applies an offset and scale factor to align a spectrum with a reference spectrum.\n",
    "\n",
    "Let's go ahead and try an MSC approach towards finding outliers. We'll create a reference spectra for each crop and stage group that is the median value at each wavelength and calculate the residual offset for each spectra relative to its reference spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:12:27.555074Z",
     "iopub.status.busy": "2022-09-24T22:12:27.554461Z",
     "iopub.status.idle": "2022-09-24T22:12:31.448733Z",
     "shell.execute_reply": "2022-09-24T22:12:31.447608Z",
     "shell.execute_reply.started": "2022-09-24T22:12:27.555035Z"
    }
   },
   "outputs": [],
   "source": [
    "def msc_fit(data, features:list=[\"Crop\", \"Stage\"]):\n",
    "    \"\"\"\n",
    "    Performs a Multiplicative Scatter Correction (MSC) to the spectra grouped\n",
    "    by the indicated columns and returns the reference spectrum along with\n",
    "    the residuals.\n",
    "    \"\"\"\n",
    "    # Compute reference spectra\n",
    "    reference = df.loc[:, [*features, *wave_cols]].groupby(features).median()\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = pandas.DataFrame(columns=[*features, *wave_cols])\n",
    "    for index, spectrum in reference.iterrows():\n",
    "        # Query database to find samples with same feature types\n",
    "        subset = data.query(\" and \".join(f\"{c} == '{index[i]}'\" for i,c in enumerate(features)))\n",
    "        # Extract the spectral columns\n",
    "        spectra = subset.loc[:, wave_cols]\n",
    "        # Perform a linear fit\n",
    "        coefs = [numpy.polyfit(spectrum, spectra.iloc[i], 1) for i in range(len(spectra))]\n",
    "        # Calculate the residuals\n",
    "        res = [numpy.poly1d(coefs[i])(spectrum.values) - spectra.iloc[i] for i in range(len(spectra))]\n",
    "        # Turn residuals into a data frame\n",
    "        res = pandas.DataFrame(res, columns=wave_cols, index=subset.index)\n",
    "        # Add the feature information\n",
    "        res.loc[:, features] = index\n",
    "        # Update the data frame\n",
    "        residuals = pandas.concat([residuals, res])\n",
    "    return reference, residuals\n",
    "                    \n",
    "reference, residuals = msc_fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and plot the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:12:31.450929Z",
     "iopub.status.busy": "2022-09-24T22:12:31.45049Z",
     "iopub.status.idle": "2022-09-24T22:12:58.938749Z",
     "shell.execute_reply": "2022-09-24T22:12:58.93739Z",
     "shell.execute_reply.started": "2022-09-24T22:12:31.450886Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "crops = labels[\"Crop\"]\n",
    "stages = labels[\"Stage\"]\n",
    "\n",
    "def get_traces_buttons():\n",
    "    traces = []\n",
    "    \n",
    "    sizes = []\n",
    "    categories = []\n",
    "    for crop in crops:\n",
    "        for stage in stages:\n",
    "            # Create traces\n",
    "            subset = residuals.query(f\"Crop=='{crop}' and Stage == '{stage}'\")\n",
    "\n",
    "            if len(subset) == 0:\n",
    "                continue\n",
    "    \n",
    "            for i, (ir, row) in enumerate(subset.iterrows()):\n",
    "                trace = go.Scatter(\n",
    "                    name=f\"ID={ir}\",\n",
    "                    legendgroup=get_label(crop,stage),\n",
    "                    x=wave,\n",
    "                    y=row[wave_cols],\n",
    "                    mode=\"markers\",\n",
    "                    marker_color=px.colors.sample_colorscale(\"viridis\", i/len(subset))[0],\n",
    "                    marker_opacity=0.5,\n",
    "                    showlegend=False,\n",
    "                )\n",
    "                traces.append(trace)\n",
    "            \n",
    "            sizes.append(len(subset))\n",
    "            categories.append(get_label(crop,stage))\n",
    "        \n",
    "    buttons = []\n",
    "    for i,cat in enumerate(categories):\n",
    "        vis = []\n",
    "        for j,_ in enumerate(categories):\n",
    "            vbool = i == j\n",
    "            vtrue = [vbool for _ in range(sizes[j])]\n",
    "            vis.extend(vtrue)\n",
    "        buttons.append(dict(\n",
    "            label=cat,\n",
    "            method=\"update\",\n",
    "            args=[dict(visible=vis)]\n",
    "        ))\n",
    "\n",
    "    return traces, buttons\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "traces, buttons = get_traces_buttons()\n",
    "fig.add_traces(traces)\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Wavelength (nm)\",\n",
    "    yaxis_title=\"Normalized Offset\",\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type=\"dropdown\",\n",
    "            direction=\"down\",\n",
    "            buttons=buttons,\n",
    "            x=0.,\n",
    "            xanchor=\"left\",\n",
    "            y=1.0,\n",
    "            yanchor=\"bottom\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.for_each_trace(lambda trace: trace.update(visible=True if trace.legendgroup == get_label(crops[0], stages[0]) else False))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a few of these classifications are starting to look a bit dubious with some major structural differences that likely represent a transition between stages. For example, the Soybean Late has some prominent spectral features over certain wavelength ranges, such as a smaller reflectance around 830 nm and 915 nm and some excess reflectance at 1050 nm and 1260 nm. Similarly Soybean Critical seems to have some spectral signature indicating a transition around 720 nm and 915 nm. Corn (Emerge_VEarly) has a lot of fluctuation indicating a transition from relatively high reflectance around 2100 - 2300 nm being associated with lower reflectance around 700 - 1300 nm and vice-versa. Sample 107 in particular appears to be a prominent Corn (Emerge_VEarly) outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "In this part we'll use a couple methods to gauge how important our features are. Additionally, it might be beneficial to normalize our data, so let's create a normalized dataset for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:12:58.940573Z",
     "iopub.status.busy": "2022-09-24T22:12:58.940223Z",
     "iopub.status.idle": "2022-09-24T22:12:58.972399Z",
     "shell.execute_reply": "2022-09-24T22:12:58.970553Z",
     "shell.execute_reply.started": "2022-09-24T22:12:58.94054Z"
    }
   },
   "outputs": [],
   "source": [
    "norm = df.loc[:,wave_cols].mean(axis=1)\n",
    "df_norm = df.loc[:,wave_cols].div(norm, axis=0)\n",
    "df_norm[\"wgt\"] = norm\n",
    "for c in df.columns:\n",
    "    if c not in df_norm:\n",
    "        df_norm[c] = df[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MANOVA\n",
    "\n",
    "From a MANOVA test we can reject the null hypothesis that there is no difference between the average feature values across the different crops and stages across a variety of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:12:58.974918Z",
     "iopub.status.busy": "2022-09-24T22:12:58.974301Z",
     "iopub.status.idle": "2022-09-24T22:12:59.92794Z",
     "shell.execute_reply": "2022-09-24T22:12:59.926691Z",
     "shell.execute_reply.started": "2022-09-24T22:12:58.974879Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "for feature in ['Crop', 'Stage']:\n",
    "    manova = MANOVA.from_formula(' + '.join(c for c in [*wave_cols, \"AEZ\", \"Month\"]) + f' ~ {feature}', df)\n",
    "    print(manova.mv_test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two-Way ANOVA\n",
    "\n",
    "Since we have two classes (Crop and Stage), using a two-way ANOVA is useful for examining how important each of the features are in distinguishing between the crop and stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:12:59.930402Z",
     "iopub.status.busy": "2022-09-24T22:12:59.929691Z",
     "iopub.status.idle": "2022-09-24T22:13:23.915718Z",
     "shell.execute_reply": "2022-09-24T22:13:23.914334Z",
     "shell.execute_reply.started": "2022-09-24T22:12:59.930358Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "def get_anova(data, cols=wave_cols):\n",
    "    df = {\n",
    "        'Crop': pandas.DataFrame(columns=[\"f\", \"p\"]),\n",
    "        'Stage': pandas.DataFrame(columns=[\"f\", \"p\"])\n",
    "    }\n",
    "\n",
    "    for c in cols:\n",
    "        model = ols(f\"{c} ~ C(Crop) + C(Stage)\", data=data).fit()\n",
    "        anova = sm.stats.anova_lm(model, typ=2)\n",
    "        for k,v in df.items():\n",
    "            v.loc[c,\"f\"] = anova.loc[f\"C({k})\"][\"F\"]\n",
    "            v.loc[c,\"p\"] = anova.loc[f\"C({k})\"][\"PR(>F)\"]\n",
    "\n",
    "    for k,v in df.items():\n",
    "        v[\"Category\"] = k\n",
    "    \n",
    "    return pandas.concat([*df.values()])\n",
    "\n",
    "for norm in [False, True]:\n",
    "    anova = get_anova(df_norm if norm else df, cols=[*wave_cols, \"wgt\"] if norm else wave_cols)\n",
    "    fig = px.bar(anova, x=anova.index, y=\"f\", color=\"Category\", barmode=\"group\")\n",
    "    fig.update_layout(title=\"Normalized\" if norm else \"Un-Normalized\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the important differences seem to lie with separating the different stages rather than the different crops. Using a normalized version results in a considerable improvement in the F-score for the Stages but not much change for the Crop types. In particular, the wavelengths around 1250 nm have about a 5x increase in the F statistic using a normalized spectrum for the stage categories.\n",
    "\n",
    "The average reflectance doesn't appear to be that important when it comes to distinguishing between the crop or stage.\n",
    "\n",
    "#### ANOVA\n",
    "\n",
    "Let's go ahead and perform a single ANOVA test to compare how the different features are at separating the Crop-Stage classification jointly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:13:23.918398Z",
     "iopub.status.busy": "2022-09-24T22:13:23.917511Z",
     "iopub.status.idle": "2022-09-24T22:13:27.082964Z",
     "shell.execute_reply": "2022-09-24T22:13:27.081829Z",
     "shell.execute_reply.started": "2022-09-24T22:13:23.918351Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_groups(data, feature):\n",
    "    categories = data[feature].unique()\n",
    "    return [data.query(f\"{feature} == '{c}'\") for c in categories]\n",
    "\n",
    "def get_joined(data):\n",
    "    crops = get_groups(data, \"Crop\").copy()\n",
    "    expand = []\n",
    "    for c in crops:\n",
    "        stages = c[\"Stage\"].unique()\n",
    "        for s in stages:\n",
    "            subset = c.query(f\"Stage == '{s}'\")\n",
    "            expand.append(subset)\n",
    "    return expand\n",
    "\n",
    "def get_anova(groups, normalized:bool=False):\n",
    "    cols = [*wave_cols, \"AEZ\", \"Month\"]\n",
    "    if normalized:\n",
    "        cols = [*cols, \"wgt\"]\n",
    "    anova = pandas.DataFrame(columns=[\"f\", \"p\"])\n",
    "    for c in cols:\n",
    "        anova.loc[c] = scipy.stats.f_oneway(*[g.loc[:,c] for g in groups])\n",
    "    return anova\n",
    "\n",
    "for norm in [False, True]:\n",
    "    data = df_norm if norm else df\n",
    "    anova_crop = get_anova(get_groups(data, \"Crop\"), normalized=norm)\n",
    "    anova_crop[\"Category\"] = \"Crop\"\n",
    "    anova_stage = get_anova(get_groups(data, \"Stage\"), normalized=norm)\n",
    "    anova_stage[\"Category\"] = \"Stage\"\n",
    "    anova_both = get_anova(get_joined(data), normalized=norm)\n",
    "    anova_both[\"Category\"] = \"Crop + Stage\"\n",
    "    anova = pandas.concat([anova_crop, anova_stage, anova_both])\n",
    "    \n",
    "    fig = px.bar(anova, y=\"f\", color=\"Category\", barmode=\"group\", labels=dict(index=\"Feature\", f=\"ANOVA F Statistic\"))\n",
    "    fig.update_layout(title=\"Normalized\" if norm else \"Un-Normalized\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The month seems to be particularly important for separating crops and stages. It would be worth investigating whether this is truly an importance feature or if our dataset is biased in a way. Crops do have different harvest times (e.g. wheat is harvested earlier than corn), so the month would likely be an important feature to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "We can also use Random Forest to gauge the relative importance of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:13:27.084641Z",
     "iopub.status.busy": "2022-09-24T22:13:27.084309Z",
     "iopub.status.idle": "2022-09-24T22:14:09.81192Z",
     "shell.execute_reply": "2022-09-24T22:14:09.810689Z",
     "shell.execute_reply.started": "2022-09-24T22:13:27.08461Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def get_rf(y, normalized:bool=False):\n",
    "    if normalized:\n",
    "        X = df_norm.loc[:,[*wave_cols, \"AEZ\", \"Month\", \"wgt\"]]\n",
    "    else:\n",
    "        X = df.loc[:,[*wave_cols, \"AEZ\", \"Month\"]]\n",
    "\n",
    "    rfc = RandomForestClassifier(n_estimators=100, random_state=9435)\n",
    "    rfc.fit(X, y)\n",
    "    return pandas.DataFrame(rfc.feature_importances_[None,:], columns=X.columns)\n",
    "\n",
    "for norm in [False, True]:\n",
    "    rfi_crop = get_rf(df[\"Crop\"], normalized=norm)\n",
    "    rfi_crop[\"Category\"] = \"Crop\"\n",
    "    rfi_stage = get_rf(df[\"Stage\"], normalized=norm)\n",
    "    rfi_stage[\"Category\"] = \"Stage\"\n",
    "    rfi_both = get_rf(df[\"Crop\"].astype('str') + df[\"Stage\"].astype('str'), normalized=norm)\n",
    "    rfi_both[\"Category\"] = \"Crop + Stage\"\n",
    "    rfi = pandas.concat([rfi_crop, rfi_stage, rfi_both])\n",
    "    \n",
    "    fig = px.bar(rfi.melt(id_vars=\"Category\"), x=\"variable\", y=\"value\", color=\"Category\", barmode=\"group\", labels=dict(variable=\"Feature\", value=\"Relative Importance\"))\n",
    "    fig.update_layout(title=\"Normalized\" if norm else \"Un-Normalized\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from random forest are quite different than our ANOVA tests, particularly in the importance of the month, AEZ, and average reflectance, which may be a result of ANOVA's assumptions being incorrect (e.g., constant variance across the classes) as well as the AEZ largely reducing the possible crop types to mostly one or two cases. There are some discrete clusters of importance appearing, such as around 700 nm, 915 nm, and 1200 nm indicating that some important spectral differences are likely occuring in these regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "Let's go ahead and create a training and testing dataset. We'll use stratified sampling, which helps preserve class balances. To ensure this applies to both the crop and stage features, we'll join the two strings together to create our class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:14:09.813519Z",
     "iopub.status.busy": "2022-09-24T22:14:09.813201Z",
     "iopub.status.idle": "2022-09-24T22:14:09.909014Z",
     "shell.execute_reply": "2022-09-24T22:14:09.908016Z",
     "shell.execute_reply.started": "2022-09-24T22:14:09.81349Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(\n",
    "    df,\n",
    "    test_size = 0.33,\n",
    "    random_state = 3852,\n",
    "    shuffle = True,\n",
    "    stratify = df[\"Crop\"].astype(str) + df[\"Stage\"].astype(str)\n",
    ")\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "train_norm = train.copy()\n",
    "test_norm = test.copy()\n",
    "\n",
    "for x in [train_norm, test_norm]:\n",
    "    x[\"wgt\"] = x.loc[:,wave_cols].mean(axis=1)\n",
    "    x.loc[:,wave_cols] = x.loc[:,wave_cols].div(x[\"wgt\"], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than utilizing something like a convolutional neural network, inputting the full spectral feature space into a machine learning model would probably be an ill-advised decision. The large number of parameters could lead to overfitting and slow performance, while the strong correlation between neighboring wavelengths suggests we could reduce our feature space considerably. In this section we'll look at a method that is well suited for our task: non-negative matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization\n",
    "\n",
    "One of the intriguing qualities about non-negative matrix factorization (NMF) is that it forces the decomposition to consist of a summation of non-negative terms. To illustrate the potential significance of this, consider the spectrum of a galaxy. The observed light would be a superposition of light from many different sources, such as the redder light from the old stellar population and bluer light from newly formed stars. As shown in the figure below, if one performs a PCA decomposition of the spectrum the resulting components will quickly bear little resemblence to any real spectral source and even have unphysical negative flux at certain wavelengths. NMF, however, tends to capture the underlying sources with the components corresponding to: (1) old stellar population, (2) newly-formed OB stars, (3) A-type stars, and (4-5) emission lines.\n",
    "\n",
    "![](http://ned.ipac.caltech.edu/level5/March19/Baron/Figures/figure14.jpg)\n",
    "\n",
    "*Decomposition of a galaxy spectra by several different methods. Figure from Vanderplas et al. (2012).*\n",
    "\n",
    "NMF works by approximating a matrix $\\bf X$ of $n$ rows (samples) and $p$ columns (features) by the product of two non-negative matrices\n",
    "$$\\bf\n",
    "X \\approx W H\n",
    "$$\n",
    "where $\\bf W$ is of dimension $n\\times c$ and $\\bf H$ is $c \\times p$ for some number of components $c \\le \\min(N,p)$\n",
    "\n",
    "One means of choosing the number of components $c$ is to introduce a spacity score that measures the complexity of the approximation:\n",
    "\n",
    "$$\n",
    "sp(c) = \\frac{n\\cdot c + c\\cdot p}{n \\cdot p} = \\frac{c}{p} + \\frac{c}{n}\n",
    "$$\n",
    "\n",
    "as well as a precision score which measures the accuracy of the approximation\n",
    "\n",
    "$$\n",
    "err(c) = \\frac{||{\\bf X} - {\\bf WH}||_{2}^{2}}{||{\\bf X}||_{2}^{2}}\n",
    "$$\n",
    "\n",
    "These scores can then be averaged to create an interpretability score between 0 and 1, with smaller values indicating a more optimal balance between simplicity and accuracy:\n",
    "\n",
    "$$\n",
    "itp(c) = 0.5 \\times \\left[\\frac{sp(c)}{\\max\\limits_{c\\in C}sp(c)} + \\frac{err(c)}{\\max\\limits_{c\\in C}err(c)} \\right]\n",
    "$$\n",
    "\n",
    "Let's go ahead and compute the precision score for several values of $c$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:46:19.723749Z",
     "iopub.status.busy": "2022-09-24T22:46:19.723274Z",
     "iopub.status.idle": "2022-09-24T22:46:51.906043Z",
     "shell.execute_reply": "2022-09-24T22:46:51.904904Z",
     "shell.execute_reply.started": "2022-09-24T22:46:19.723686Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "n_components = numpy.arange(2,9,1)\n",
    "\n",
    "def err_score(model, X, y=None):\n",
    "    X_pred = model.inverse_transform(model.transform(X))\n",
    "    return numpy.linalg.norm(X - X_pred)**2 / numpy.linalg.norm(X)**2\n",
    "    \n",
    "def compute_scores(data, n_components):\n",
    "    X = data.loc[:, wave_cols]\n",
    "    \n",
    "    scores = []\n",
    "    for n in tqdm(n_components):\n",
    "        nmf = NMF(n_components=n, max_iter=1000, init='nndsvda')\n",
    "        cvs = cross_val_score(nmf, X, cv=5, scoring=err_score)\n",
    "        scores.append(numpy.mean(cvs))\n",
    "\n",
    "    return scores\n",
    "                      \n",
    "scores = compute_scores(train_norm, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now plot up the interpretability score for each set of components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:15:08.005746Z",
     "iopub.status.busy": "2022-09-24T22:15:08.005017Z",
     "iopub.status.idle": "2022-09-24T22:15:08.123843Z",
     "shell.execute_reply": "2022-09-24T22:15:08.122607Z",
     "shell.execute_reply.started": "2022-09-24T22:15:08.005669Z"
    }
   },
   "outputs": [],
   "source": [
    "def interp(X, n_components, error):\n",
    "    sp = n_components / max(n_components)\n",
    "    err = error / max(error)\n",
    "    return 0.5 * (sp + err)\n",
    "\n",
    "iscores = interp(train, n_components, scores)\n",
    "fig = px.scatter(x=n_components, y=iscores)\n",
    "fig.update_layout(xaxis_title=\"# Components\", yaxis_title=\"CV Score (Interpretability)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 5 components is our ideal number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:15:08.126365Z",
     "iopub.status.busy": "2022-09-24T22:15:08.125277Z",
     "iopub.status.idle": "2022-09-24T22:15:10.759907Z",
     "shell.execute_reply": "2022-09-24T22:15:10.758748Z",
     "shell.execute_reply.started": "2022-09-24T22:15:08.126319Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=n_components[numpy.argmin(iscores)], max_iter=1000, init='nndsvda')\n",
    "nmf.fit(train_norm.loc[:,wave_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and plot our NMF component spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:15:10.762024Z",
     "iopub.status.busy": "2022-09-24T22:15:10.761312Z",
     "iopub.status.idle": "2022-09-24T22:15:10.786625Z",
     "shell.execute_reply": "2022-09-24T22:15:10.785479Z",
     "shell.execute_reply.started": "2022-09-24T22:15:10.76198Z"
    }
   },
   "outputs": [],
   "source": [
    "components = nmf.components_.reshape(-1, len(wave_cols))\n",
    "components = components / components.mean(axis=1, keepdims=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "for i,component in enumerate(components):\n",
    "    fig.add_trace(go.Scatter(x=wave, y=component, name=f\"PC{i}\"))\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Wavelength (nm)\",\n",
    "    yaxis_title=\"Relative Surface Reflectance\",\n",
    "    legend_title=\"Component\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is a lot of importance features located in the 800 - 1200 nm range that our different components are trying to capture, which is where chorophyll is most active.\n",
    "\n",
    "Let's compare our components with the average crop and stage to see if they bear any resemblance to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:15:10.789002Z",
     "iopub.status.busy": "2022-09-24T22:15:10.788308Z",
     "iopub.status.idle": "2022-09-24T22:15:10.877587Z",
     "shell.execute_reply": "2022-09-24T22:15:10.876495Z",
     "shell.execute_reply.started": "2022-09-24T22:15:10.78896Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_plot(data, feature:str):\n",
    "    avg_spec = data.loc[:,[feature, *wave_cols]].groupby(feature).mean()\n",
    "    \n",
    "    traces = []\n",
    "    # Add the averaged features\n",
    "    for i in range(len(avg_spec)):\n",
    "        trace = go.Scatter(x=wave, y=avg_spec.iloc[i,:], name=avg_spec.index[i].capitalize().replace(\"_\", \" \"), legendgroup=feature.capitalize())\n",
    "        traces.append(trace)\n",
    "    \n",
    "    # Add the components\n",
    "    for i,component in enumerate(components):\n",
    "        trace = go.Scatter(x=wave, y=component, name=f\"PC{i}\", visible=True if i == 0 else False, legendgroup=\"NMF\", marker=dict(color=plotly.colors.qualitative.Plotly[len(avg_spec)]))\n",
    "        traces.append(trace)\n",
    "\n",
    "    # Buttons to make one component visible at a time\n",
    "    n = len(components)\n",
    "    visible = [True for _ in range(len(avg_spec))] # Keep average feature spectra visible\n",
    "    visible = [[*visible, *[True if i==j else False for i in range(n)]] for j in range(n)] # Toggle components\n",
    "\n",
    "    buttons = [\n",
    "        dict(\n",
    "            label=f\"PC{i}\",\n",
    "            method=\"update\",\n",
    "            args=[dict(visible=v)]\n",
    "        )\n",
    "        for i,v in enumerate(visible)\n",
    "    ]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_traces(traces)\n",
    "    fig.update_layout(\n",
    "        xaxis_title = \"Wavelength (nm)\",\n",
    "        yaxis_title = \"Relative Surface Reflectance\",\n",
    "        #legend_title = feature2,\n",
    "        #yaxis_range = [0,100],\n",
    "        updatemenus = [dict(\n",
    "            type=\"buttons\",\n",
    "            direction=\"right\",\n",
    "            buttons=buttons,\n",
    "            x=0., xanchor=\"left\",\n",
    "            y=1., yanchor=\"bottom\"\n",
    "        )]\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "make_plot(train_norm, \"Crop\")\n",
    "make_plot(train_norm, \"Stage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our last component tends to capture the general shape of the averaged spectra while the remaining ones are latent features that affect the spectrum.\n",
    "\n",
    "Let's go ahead and transform our training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:15:10.88016Z",
     "iopub.status.busy": "2022-09-24T22:15:10.879418Z",
     "iopub.status.idle": "2022-09-24T22:15:10.9691Z",
     "shell.execute_reply": "2022-09-24T22:15:10.967842Z",
     "shell.execute_reply.started": "2022-09-24T22:15:10.880117Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf_cols = [f\"PC{i}\" for i in range(len(nmf.components_))]\n",
    "train_nmf = pandas.DataFrame(nmf.transform(train_norm.loc[:,wave_cols]), columns=nmf_cols)\n",
    "test_nmf = pandas.DataFrame(nmf.transform(test_norm.loc[:,wave_cols]), columns=nmf_cols)\n",
    "\n",
    "for c in [\"AEZ\", \"Month\", \"wgt\"]:\n",
    "    train_nmf[c] = train_norm[c]\n",
    "    test_nmf[c] = test_norm[c]\n",
    "\n",
    "for col in [\"Crop\", \"Stage\"]:\n",
    "    train_nmf[col] = train[col]\n",
    "    test_nmf[col] = test[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and visualize the crop and stage distributions based on the NMF components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:15:10.972102Z",
     "iopub.status.busy": "2022-09-24T22:15:10.971243Z",
     "iopub.status.idle": "2022-09-24T22:15:11.17483Z",
     "shell.execute_reply": "2022-09-24T22:15:11.173761Z",
     "shell.execute_reply.started": "2022-09-24T22:15:10.972056Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"Crop\": list(sorted(df[\"Crop\"].unique())),\n",
    "    \"Stage\": stage_sequence.categories\n",
    "}\n",
    "\n",
    "def make_splom(feature):\n",
    "    traces = []\n",
    "    cols = nmf_cols\n",
    "    for fi, f in enumerate(labels[feature]):\n",
    "        subset = train_nmf.query(f\"{feature}=='{f}'\")\n",
    "\n",
    "        trace = go.Splom(\n",
    "            name=f,\n",
    "            legendgroup=f,\n",
    "            dimensions=[dict(\n",
    "                label=col,\n",
    "                values=subset[col]\n",
    "            ) for col in [*cols, feature]],\n",
    "            marker=dict(color=plotly.colors.qualitative.Plotly[fi]),\n",
    "            diagonal_visible=False,\n",
    "            showupperhalf=False,\n",
    "        )\n",
    "        traces.append(trace)\n",
    "        \n",
    "    fig = go.Figure()\n",
    "    fig.add_traces(traces)\n",
    "    fig.update_layout(width=800, height=800, legend=dict(title=feature, xanchor=\"right\", x=1))\n",
    "    fig.show()\n",
    "\n",
    "make_splom(\"Crop\")\n",
    "make_splom(\"Stage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying with XGBoost\n",
    "\n",
    "One of the most powerful machine learning methods is [XGBoost](https://arxiv.org/pdf/1603.02754.pdf), which is a tree-based learning model. While random forest focuses on creating numerous independent trees to reduce the variance and overfitting, XGBoost trains the trees sequentially such that each tree is trained to minimize the residuals from the previous tree resulting in a more powerful learner that reduces bias and underfitting. \n",
    "\n",
    "Tree-based learning involves the creation of a model to generate a prediction $\\hat{y}_{i}$ given a set of features $\\boldsymbol{x}_{i}$ associated with an observation. If there are $K$ additive functions, then the prediction is given by\n",
    "\n",
    "$$\n",
    "\\hat{y}_{i} = \\sum_{k=1}^{K} f_{k}(\\boldsymbol{x}_i), \\qquad f_k \\in \\mathcal{F}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{F} = \\{f_{k}(\\boldsymbol{x}) = w_{q(\\boldsymbol{x})}\\}(q : \\mathbb{R} \\rightarrow T, w \\in \\mathbb{R}^{T})$ is the space of regression trees, $f_{k}$ a function in the space $\\mathcal{F}$, $q$ the structure of each tree, $T$ the number of leaves in each tree, and $w_i$ the weight associated with leaf $i$. The decision tree rules are specified by $q$, which are used to assign a unique leaf in each tree to the input. Finally, a weighted sum of the leaves are used to derive the final prediction. XGBoost uses additative training such that\n",
    "\n",
    "$$\n",
    "\\hat{y}_{i}^{(k)} = \\hat{y}_{i}^{(k-1)} + f_{k}(\\boldsymbol{x}_{i})\n",
    "$$\n",
    "\n",
    "Learning the decision rules $q$ and leaf weights $w$ involves minimizing the objective loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y, \\hat{y}) = \\sum_{i=1}^{n} \\ell(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "where the first term is the training loss (how well the model fits the data) and the second term is a regularization loss to favor simpler models over more complex ones and reduce overfitting. XGBoost uses the following regularization term\n",
    "\n",
    "$$\n",
    "\\Omega(f_t) = \\gamma T + \\frac{1}{2}\\alpha ||w||_{1} + \\frac{1}{2}\\lambda ||w||_{2}^2\n",
    "$$\n",
    "\n",
    "where the first part regularizes the number of of leaves $T$ and the remaining parts regularize the weights using the L1 and L2 norms respectively. Since it is not practical to search the entire space, greedy based approaches are used to construct the tree structures.\n",
    "\n",
    "While there are a variety of parameters associated with XGBoost that need to be fine-tune for optimal results, let's keep it simple and just consider the regularization weights $\\alpha$ and $\\lambda$. We'll use a grid search and apply cross-validation to choose the optimal number of parameters. Let's create a function for performing this search optimization and calculating the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:15:11.177128Z",
     "iopub.status.busy": "2022-09-24T22:15:11.176302Z",
     "iopub.status.idle": "2022-09-24T22:15:11.693839Z",
     "shell.execute_reply": "2022-09-24T22:15:11.692902Z",
     "shell.execute_reply.started": "2022-09-24T22:15:11.177083Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def get_labels(data):\n",
    "    return data.apply(lambda row: row[\"Crop\"] + \"_\" + row[\"Stage\"], axis=1)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(get_labels(train_norm))\n",
    "\n",
    "def get_input(data, wave_cols):\n",
    "    X_train = data.loc[:,[*wave_cols, \"AEZ\", \"Month\", \"wgt\"]]\n",
    "    return X_train, label_encoder.transform(get_labels(data))\n",
    "\n",
    "def xgboost_grid_search(data, wave_cols, param_grid):\n",
    "    X_train, y_train = get_input(data, wave_cols)\n",
    "\n",
    "    search = GridSearchCV(XGBClassifier(), param_grid=param_grid, cv=3, verbose=3)\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    print(search.best_params_)\n",
    "    xgb = XGBClassifier(**search.best_params_)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    \n",
    "    return xgb\n",
    "\n",
    "def get_confusion_matrix(target, prediction):\n",
    "    cmat = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "    cmat = {\n",
    "        \"Confusion Matrix\": cmat,\n",
    "        \"Precision Matrix, P(Y|X)\": cmat / cmat.sum(axis=0, keepdims=True), # Up+Down = P(Y|X)\n",
    "        \"Recall Matrix, P(X|Y)\": cmat / cmat.sum(axis=1, keepdims=True) # Left+Right = P(X|Y)\n",
    "    }\n",
    "    \n",
    "    return {k:pandas.DataFrame(v, columns=label_encoder.classes_, index=label_encoder.classes_) for k,v in cmat.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Spectra\n",
    "\n",
    "For our first model let's make use of our entire spectra and later reproduce our analysis on the NMF reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:15:11.695421Z",
     "iopub.status.busy": "2022-09-24T22:15:11.695109Z",
     "iopub.status.idle": "2022-09-24T22:41:05.131162Z",
     "shell.execute_reply": "2022-09-24T22:41:05.129743Z",
     "shell.execute_reply.started": "2022-09-24T22:15:11.695392Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lambda': [0, 0.1, 1, 10],\n",
    "    'alpha': [0, 0.1, 1, 10],\n",
    "}\n",
    "  \n",
    "xgb = xgboost_grid_search(train_norm, wave_cols, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model now trained let's go ahead and test its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:41:05.133615Z",
     "iopub.status.busy": "2022-09-24T22:41:05.13317Z",
     "iopub.status.idle": "2022-09-24T22:41:05.561391Z",
     "shell.execute_reply": "2022-09-24T22:41:05.5603Z",
     "shell.execute_reply.started": "2022-09-24T22:41:05.133573Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test = get_input(test_norm, wave_cols)\n",
    "test_pred = xgb.predict(X_test)\n",
    "\n",
    "cmat = get_confusion_matrix(y_test, test_pred)\n",
    "\n",
    "print(\"Accuracy: \", numpy.diag(cmat[\"Confusion Matrix\"]).sum().sum() / cmat[\"Confusion Matrix\"].sum().sum())\n",
    "\n",
    "for k,v in cmat.items():\n",
    "    fig = px.imshow(v)\n",
    "    fig.update_layout(title=k, xaxis_title=\"Predicted\", yaxis_title=\"Ground Truth\", width=800, height=800)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got about 87% accuracy. Looks like the early stages of corn and soybeans are particular difficult to distinguish, which isn't that surprising. Many of the other common errors are with nearby stages of the same crop, although winter wheat and cotton seem to have a fair bit of overlap along with rice and corn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF Spectral Decomposition\n",
    "\n",
    "Let's go ahead and redo our analysis using our NMF transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:41:05.563443Z",
     "iopub.status.busy": "2022-09-24T22:41:05.563018Z",
     "iopub.status.idle": "2022-09-24T22:44:13.816754Z",
     "shell.execute_reply": "2022-09-24T22:44:13.815679Z",
     "shell.execute_reply.started": "2022-09-24T22:41:05.563402Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lambda': [0, 0.1, 1, 10],\n",
    "    'alpha': [0, 0.1, 1, 10],\n",
    "}\n",
    "  \n",
    "xgb_nmf = xgboost_grid_search(train_nmf, nmf_cols, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we are getting about 10x quicker training time, so that's a huge improvement. Let's go ahead and look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T22:44:50.733171Z",
     "iopub.status.busy": "2022-09-24T22:44:50.732782Z",
     "iopub.status.idle": "2022-09-24T22:44:50.967985Z",
     "shell.execute_reply": "2022-09-24T22:44:50.966908Z",
     "shell.execute_reply.started": "2022-09-24T22:44:50.733139Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test = get_input(test_nmf, nmf_cols)\n",
    "test_pred = xgb_nmf.predict(X_test)\n",
    "\n",
    "cmat = get_confusion_matrix(y_test, test_pred)\n",
    "\n",
    "print(\"Accuracy: \", numpy.diag(cmat[\"Confusion Matrix\"]).sum().sum() / cmat[\"Confusion Matrix\"].sum().sum())\n",
    "\n",
    "for k,v in cmat.items():\n",
    "    fig = px.imshow(v)\n",
    "    fig.update_layout(title=k, xaxis_title=\"Predicted\", yaxis_title=\"Ground Truth\", width=800, height=800)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy recreased by about 2% down to 85%. Many of the errors are quite similar between the two, but overall NMF appears to provide a nice approximation to our spectra."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1870056,
     "sourceId": 3054434,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30260,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
